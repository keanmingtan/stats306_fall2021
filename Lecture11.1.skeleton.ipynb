{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11.1: Statistical Modeling\n",
    "<div style=\"border: 1px double black; padding: 10px; margin: 10px\">\n",
    "From now until the end of the course we will focus on statistical modeling ([Part IV] of your book).\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.2     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.0.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.1     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a model\n",
    "A statistical model is a mathematical formula that relates an outcome with one or more explanatory variables.\n",
    "\n",
    "$$\\underbrace{Y}_{\\text{outcome}} = \\underbrace{f}_{\\text{model function}}(\\underbrace{X}_{\\text{explainer}}) + \n",
    "    \\underbrace{\\epsilon}_\\text{noise}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model classes\n",
    "The types of functions $f$ that we allow determine what is called the *model class*. For example, in STATS 250 you learned about linear regression, where $f$ is any function of the form \n",
    "\n",
    "$$f(x) = a_1 + a_2x $$\n",
    "\n",
    "for some *parameters* $a_1$ and $a_2$. This defines a whole *family* of models: one for each choice of slope and intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "The process of *fitting* a model refers to selecting the particular choice $\\hat{a},\\hat{b}$ from the family of models that we have chosen, in order to best fit the data. The fitted model is the member of the model family we have selected that is \"closest\" to the data. This *does not* mean that this is the \"true\" model! In most cases there is no \"true\" model. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.\n",
    "\n",
    "### Model selection\n",
    "There is also the question of which family of models to use. In other words, which types of functions $f(x)$ to use. To use a fashionable example, we could have instead chosen our model family to be \n",
    "\n",
    "$$\\{f: \\text{$f$ is a neural network}\\}.$$ \n",
    "\n",
    "The problem of choosing a model family is known as *model selection*. It is a much trickier problem than model fitting because there is no one correct answer: \"all models are wrong\"; the appropriate model family balances our needs for interpretability, predictiveness, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "We'll use the `modelr` package (again part of tidyverse) to learn about modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `modelr` package comes with a simple bivariate dataset that we can model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to explore the data using the techniques we have already learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong linear relationship. We suspect that a good model might be the one we saw above: $y = a_1 + a_2x$. If we select a particular $a$ and $b$, this gives us a potential model for the data. We can plot this for various choices of $a_1$ and $a_2$ and see visually see how well it might fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line represents the value of $y$ that we would predict for each value of $x$. To measure how good our model fit is we can do the following: for each pair of data points $(x_i,y_i)$, measure the distance $|\\hat{y}_i - y_i|$ between predicted and observed values of $y_i$. The value $\\hat{y}_i - y_i$ is called the *residual*. It's the component of the data that isn't predicted by our model. Adding up the residuals gives us a measure of how good our model fits the data. If we predict the data perfectly ($\\hat{y}_i = y_i$ for all $i$) then this would equal zero, so lower values are better. (Later you will learn that this is only true up to a point; it is generally not a good idea to fit the data perfectly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a model from this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function which takes a model and plots the residuals for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I have jittered the data slightly to make the regression effect more apparent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how well our model peforms we define a function which adds up the square of the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to figure out what is the best fitting model, i.e. the choice of $a$ that minimizes `measure_distance` as defined above. Let's first try simply generating a bunch of random models and seeing which one has the smallest `measure_distance`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the three best-fitting models (the three indices $i$ for which `sim1_dist(a1[i], a2[i])` is smallest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the model fits by plotting `dist[i]` for each pair `c(a1[i], a2[i])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than exploring the models randomly let us systematically try all points on a grid of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat the same plot, we see that the best fitting models are roughly centered around `a=c(4,2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot these models they all fit the data pretty well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the \"best\" model you could imagine taking a finer and finer grid of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this \"brute force\" approach is wasteful. We can do much better by using an optimization algorithm to find the minimum for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization in R\n",
    "Optimization means \"find the minimum of a function\". In R the command `optim` can be used to optimize a function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `optim` to find the values `a1` and `a2` that minimize `sim1_dist`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `lm` command\n",
    "The simple linear model class we are considering is a special case of the larger family\n",
    "\n",
    "$$y = a_0 + a_1 x_1 + \\dots + a_p x_p.$$\n",
    "\n",
    "R has a special commmand `lm` used to fit this type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are the same parameter estimates that we got using `optim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing models\n",
    "In this section we will learn some ways to visualize statistical models. One way is to look at the *predictions* made by the model over the observed range of values for our explanatory variable(s). The command `modelr::data_grid` will generate this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;246m# A tibble: 30 x 2\u001b[39m\n",
      "       x     y\n",
      "   \u001b[3m\u001b[38;5;246m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[38;5;250m 1\u001b[39m     1  4.20\n",
      "\u001b[38;5;250m 2\u001b[39m     1  7.51\n",
      "\u001b[38;5;250m 3\u001b[39m     1  2.13\n",
      "\u001b[38;5;250m 4\u001b[39m     2  8.99\n",
      "\u001b[38;5;250m 5\u001b[39m     2 10.2 \n",
      "\u001b[38;5;250m 6\u001b[39m     2 11.3 \n",
      "\u001b[38;5;250m 7\u001b[39m     3  7.36\n",
      "\u001b[38;5;250m 8\u001b[39m     3 10.5 \n",
      "\u001b[38;5;250m 9\u001b[39m     3 10.5 \n",
      "\u001b[38;5;250m10\u001b[39m     4 12.4 \n",
      "\u001b[38;5;246m# … with 20 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sim1 %>% print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add the predictions for each $x$ value in `grid`:v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot these predictions using `ggplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the advantage of using `modelr` here versus just extracting the coefficients and doing it ourselves, as before? The `modelr` code works with any model. We could have used something more complicated for `sim1_mod`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals\n",
    "Each data point $y_i$ can be decomposed as \n",
    "\n",
    "$$y_i = \\underbrace{\\hat{y}_i}_\\text{prediction} + \\underbrace{\\epsilon_i}_\\text{residual}.$$\n",
    "\n",
    "The prediction is the pattern in the data that the model has captured, and the residual is what is left over. To explore the residuals, use `add_residuals()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from STATS 250, the linear model assumes that the residuals $\\epsilon_i$ are independent and normally distributed. By visualizing the residuals, we may judge whether this assumption holds or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model has done a good job of capturing patterns in the data, then the residuals should look like random noise. (In other words, if the residuals contain obvious patterns, then there is more modeling work to be done!) You should confirm this by visualizing the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas\n",
    "We have seen examples of formulas when working with the `facet_*()` commands, and also used them in place of anonymous functions with `map`. Formulas are most commonly used in model fitting commands like `lm`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the notation `y ~ x` cause R to fit the model \n",
    "\n",
    "$$y = a_1 + a_2 x.$$\n",
    "\n",
    "The command `model_matrix` can show us exactly how formulas work on specific data examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;246m# A tibble: 3 x 3\u001b[39m\n",
      "      y    x1    x2\n",
      "  \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[38;5;250m1\u001b[39m     4     2     5\n",
      "\u001b[38;5;250m2\u001b[39m     5     1     6\n",
      "\u001b[38;5;250m3\u001b[39m     1     2     3\n"
     ]
    }
   ],
   "source": [
    " df <- tribble(\n",
    "   ~y, ~x1, ~x2,\n",
    "   4, 2, 5,\n",
    "   5, 1, 6,\n",
    "   1, 2, 3\n",
    " ) %>% print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, R will add an intercept term. If you want to fit a model with no intercept, you should subtract 1 from the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
